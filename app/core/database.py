import asyncpg
import asyncio
import os
import logging
from typing import Optional, Union
from pathlib import Path
from dotenv import load_dotenv

# Import Supabase client
from .supabase_client import supabase_db, SupabaseDatabase

load_dotenv()
logger = logging.getLogger(__name__)

# Environment configuration
FASTAPI_ENV = os.getenv("FASTAPI_ENV", "local").lower()

# Database configuration for local environment
DATABASE_URL = f"postgresql://{os.getenv('POSTGRES_USER', 'opinator')}:{os.getenv('POSTGRES_PASSWORD', 'opinator123')}@{os.getenv('POSTGRES_HOST', 'localhost')}:{os.getenv('POSTGRES_PORT', '5435')}/{os.getenv('POSTGRES_DB', 'opinator')}"

class Database:
    def __init__(self):
        self.pool: Optional[asyncpg.Pool] = None

    async def connect(self):
        """Establish database connection"""
        try:
            self.pool = await asyncpg.create_pool(
                DATABASE_URL,
                min_size=1,
                max_size=10
            )
            # Extract host and port for safe logging
            host = os.getenv('POSTGRES_HOST', 'localhost')
            port = os.getenv('POSTGRES_PORT', '5435')
            db_name = os.getenv('POSTGRES_DB', 'opinator')
            print(f"‚úÖ Connected to PostgreSQL at {host}:{port}/{db_name}")

            # Initialize tables if they don't exist
            await self.initialize_database()
            return True
        except Exception as e:
            print(f"‚ùå Error connecting to PostgreSQL: {e}")
            return False

    async def initialize_database(self):
        """Execute environment-specific SQL to create tables and initial data, then run migrations"""
        try:
            # Determine which SQL file to use based on environment
            sql_base_path = Path(__file__).parent.parent.parent / "sql"

            if FASTAPI_ENV == "production":
                # For production, use supabase_init.sql (though this should not be reached in normal flow)
                init_sql_path = sql_base_path / "supabase_init.sql"
                logger.info("üîß Using Supabase-compatible SQL for local PostgreSQL (testing mode)")
            else:
                # For local development, use traditional init.sql
                init_sql_path = sql_base_path / "init.sql"
                logger.info("üîß Using local PostgreSQL init.sql")

            if init_sql_path.exists():
                with open(init_sql_path, 'r', encoding='utf-8') as f:
                    init_sql = f.read()

                # For PostgreSQL, we need to adapt the Supabase SQL if needed
                if FASTAPI_ENV == "production" and "supabase_init" in str(init_sql_path):
                    init_sql = self._adapt_supabase_sql_for_postgres(init_sql)

                async with self.pool.acquire() as connection:
                    # Execute the SQL in chunks for complex scripts
                    await self._execute_sql_script(connection, init_sql)

                logger.info("üèóÔ∏è  Database initialized successfully")
            else:
                logger.warning(f"‚ö†Ô∏è  SQL file not found at {init_sql_path}")

            # Run migrations
            await self.run_migrations()

        except Exception as e:
            logger.error(f"‚ö†Ô∏è  Error initializing database: {e}")
            # Not critical, tables might already exist

    def _adapt_supabase_sql_for_postgres(self, sql_content: str) -> str:
        """Adapt Supabase SQL for local PostgreSQL compatibility"""
        # Replace Supabase-specific syntax with PostgreSQL equivalents
        adapted_sql = sql_content

        # Replace BIGINT GENERATED BY DEFAULT AS IDENTITY with SERIAL
        adapted_sql = adapted_sql.replace(
            'BIGINT GENERATED BY DEFAULT AS IDENTITY',
            'SERIAL'
        )

        # Replace TIMESTAMP WITH TIME ZONE with TIMESTAMP
        adapted_sql = adapted_sql.replace(
            'TIMESTAMP WITH TIME ZONE DEFAULT NOW()',
            'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'
        )

        # Remove RLS comments and Supabase-specific features
        lines = adapted_sql.split('\n')
        filtered_lines = []
        for line in lines:
            if 'ROW LEVEL SECURITY' in line or 'rpc(' in line:
                continue  # Skip Supabase-specific lines
            filtered_lines.append(line)

        adapted_sql = '\n'.join(filtered_lines)
        logger.info("üîß Adapted Supabase SQL for PostgreSQL compatibility")
        return adapted_sql

    async def _execute_sql_script(self, connection, sql_content: str):
        """Execute SQL script safely, handling multiple statements"""
        try:
            # Split by semicolon and execute statements individually
            statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]

            for i, statement in enumerate(statements):
                if statement and not statement.startswith('--'):
                    try:
                        await connection.execute(statement)
                        logger.debug(f"Executed statement {i+1}/{len(statements)}")
                    except Exception as e:
                        logger.warning(f"Statement {i+1} failed (may be expected): {str(e)[:100]}")

        except Exception as e:
            logger.error(f"Error executing SQL script: {e}")
            raise

    async def run_migrations(self):
        """Run migration files in order"""
        try:
            migrations_dir = Path(__file__).parent.parent.parent / "sql" / "migrations"
            if not migrations_dir.exists():
                return

            # Get all migration files and sort them
            migration_files = sorted([f for f in migrations_dir.glob("*.sql")])

            if not migration_files:
                return

            migration_count = 0
            for migration_file in migration_files:
                try:
                    with open(migration_file, 'r', encoding='utf-8') as f:
                        migration_sql = f.read()

                    async with self.pool.acquire() as connection:
                        await connection.execute(migration_sql)
                    migration_count += 1

                except Exception as e:
                    print(f"‚ö†Ô∏è  Migration {migration_file.name} failed: {e}")
                    # Continue with other migrations

            if migration_count > 0:
                print(f"üì¶  Applied {migration_count} database migrations")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error running migrations: {e}")

    async def disconnect(self):
        """Close database connection"""
        if self.pool:
            await self.pool.close()
            print("üîå PostgreSQL connection closed")

    async def execute_query(self, query: str, *args):
        """Execute a query that doesn't return results"""
        async with self.pool.acquire() as connection:
            return await connection.execute(query, *args)

    async def fetch_query(self, query: str, *args):
        """Execute a query that returns multiple results"""
        async with self.pool.acquire() as connection:
            return await connection.fetch(query, *args)

    async def fetch_one(self, query: str, *args):
        """Execute a query that returns one result"""
        async with self.pool.acquire() as connection:
            return await connection.fetchrow(query, *args)

    async def create_scraping_job(self, search_query: str, search_type: str, platforms: list):
        """Create a new scraping job"""
        query = """
        INSERT INTO scraping_jobs (search_query, search_type, platforms, status)
        VALUES ($1, $2, $3, 'pending')
        RETURNING id
        """
        result = await self.fetch_one(query, search_query, search_type, platforms)
        return result['id'] if result else None

    async def update_job_status(self, job_id: int, status: str, error_message: str = None):
        """Update job status"""
        if status == 'completed':
            query = """
            UPDATE scraping_jobs
            SET status = $2, completed_at = CURRENT_TIMESTAMP
            WHERE id = $1
            """
            await self.execute_query(query, job_id, status)
        else:
            query = """
            UPDATE scraping_jobs
            SET status = $2, error_message = $3
            WHERE id = $1
            """
            await self.execute_query(query, job_id, status, error_message)

    async def save_review(self, job_id: int, platform: str, review_data: dict):
        """Save a review to the database"""
        query = """
        INSERT INTO reviews (
            job_id, platform, review_id, rating, review_text,
            author_name, review_date, helpful_votes, source_url, raw_data
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        """
        await self.execute_query(
            query,
            job_id,
            platform,
            review_data.get('review_id'),
            review_data.get('rating'),
            review_data.get('text'),
            review_data.get('author'),
            review_data.get('date'),
            review_data.get('helpful_votes', 0),
            review_data.get('source_url'),
            review_data  # raw_data as JSON
        )

    async def get_platform_selectors(self, platform: str):
        """Get CSS selectors for a platform"""
        query = "SELECT selectors FROM platform_configs WHERE platform = $1 AND is_active = true"
        result = await self.fetch_one(query, platform)
        return result['selectors'] if result else {}

    async def get_job_reviews(self, job_id: int):
        """Get all reviews for a job"""
        query = """
        SELECT platform, rating, review_text, author_name, review_date, helpful_votes
        FROM reviews
        WHERE job_id = $1
        ORDER BY scraped_at DESC
        """
        return await self.fetch_query(query, job_id)

class UnifiedDatabase:
    """Unified database interface that works with both local PostgreSQL and Supabase"""

    def __init__(self):
        self.env = FASTAPI_ENV
        # Always use Supabase regardless of environment
        self.supabase_db = supabase_db
        self.active_db = None

    async def connect(self):
        """Connect to Supabase for all environments"""
        logger.info(f"üåç Environment: {self.env}")

        if self.supabase_db:
            logger.info("üîó Connecting to Supabase")
            success = await self.supabase_db.connect()
            if success:
                self.active_db = self.supabase_db
            return success
        else:
            logger.error("‚ùå Supabase not available")
            return False

    async def disconnect(self):
        """Disconnect from active database"""
        if self.active_db:
            await self.active_db.disconnect()
            self.active_db = None

    # Proxy methods to active database
    async def execute_query(self, query: str, *args):
        if self.active_db:
            return await self.active_db.execute_query(query, *args)
        return None

    async def fetch_query(self, query: str, *args):
        if self.active_db:
            return await self.active_db.fetch_query(query, *args)
        return []

    async def fetch_one(self, query: str, *args):
        if self.active_db:
            return await self.active_db.fetch_one(query, *args)
        return None

    async def create_scraping_job(self, search_query: str, search_type: str, platforms: list):
        if self.active_db:
            return await self.active_db.create_scraping_job(search_query, search_type, platforms)
        return None

    async def update_job_status(self, job_id: int, status: str, error_message: str = None):
        if self.active_db:
            return await self.active_db.update_job_status(job_id, status, error_message)
        return False

    async def save_review(self, job_id: int, platform: str, review_data: dict):
        if self.active_db:
            return await self.active_db.save_review(job_id, platform, review_data)
        return False

    async def get_job_reviews(self, job_id: int):
        if self.active_db:
            return await self.active_db.get_job_reviews(job_id)
        return []

    @property
    def pool(self):
        """For backward compatibility with asyncpg pool access"""
        if self.env == "local" and self.active_db:
            return self.active_db.pool
        return None

    def is_supabase(self):
        """Check if we're using Supabase"""
        return self.active_db and hasattr(self.active_db, 'client')

    def get_supabase_client(self):
        """Get Supabase client if available"""
        if self.is_supabase():
            return self.active_db.client
        return None

    async def execute_unified_query(self, query: str, *args, fetch_mode='all'):
        """Execute query in both PostgreSQL and Supabase environments

        Args:
            query: SQL query string
            args: Query parameters
            fetch_mode: 'all', 'one', or 'execute'
        """
        try:
            if hasattr(self, 'pool') and self.pool:
                # PostgreSQL local
                async with self.pool.acquire() as connection:
                    if fetch_mode == 'all':
                        result = await connection.fetch(query, *args)
                        return [dict(row) for row in result]
                    elif fetch_mode == 'one':
                        result = await connection.fetchrow(query, *args)
                        return dict(result) if result else None
                    else:  # execute
                        return await connection.execute(query, *args)
            else:
                # Supabase - return empty for raw SQL queries (needs table operations)
                return [] if fetch_mode == 'all' else None
        except Exception as e:
            logger.error(f"‚ùå Error executing unified query: {e}")
            return [] if fetch_mode == 'all' else None


# Global unified database instance
db = UnifiedDatabase()

async def init_database():
    """Initialize database connection"""
    success = await db.connect()
    if not success:
        logger.warning("‚ö†Ô∏è  Running without database - demo mode only")
    return success

async def close_database():
    """Close database connection"""
    await db.disconnect()